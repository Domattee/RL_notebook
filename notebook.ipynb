{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2021 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "To render video you will need to have ffmpeg and xvbf installed.\n",
        "Typically, installing is done with the command\n",
        "\n",
        "`sudo apt-get install -y xvfb ffmpeg`\n",
        "\n",
        "Then, if you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym[classic_control]\n",
        "!pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "source": [
        "# env_name = 'CartPole-v0'                # discrete action space\n",
        "# env_name = 'LunarLander-v2'             # discrete action space\n",
        "env_name = 'LunarLanderContinuous-v2'   # continuous action space\n",
        "# env_name = 'BipedalWalker-v3'           # continuous action space\n",
        "# env_name = 'BipedalWalkerHardcore-v3'     # continuous action space\n",
        "env = gym.make(env_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "You can render this environment to see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.high)\n",
        "print(env.observation_space.low)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1OEsjD43N1Y"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.reward_range)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "    \n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "\n",
        "        observation = environment.reset()\n",
        "        episode_return = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy(observation)\n",
        "            observation, reward, done, info = environment.step(action)\n",
        "            episode_return += reward\n",
        "            total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-K4TSs3N1a"
      },
      "source": [
        "def random_policy(observation):\n",
        "    return env.action_space.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrIwVRj_Q94D"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "from functools import partial\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZiOx6lQ7Gq"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(input_size, input_size, bias=True)\n",
        "        self.l2 = nn.Linear(input_size, input_size, bias=True)\n",
        "        #nn.init.normal_(self.linear.weight, mean = 0.0, std = sqrt(1/input_size))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.l1(x)\n",
        "        scores = self.relu(scores)\n",
        "        scores = self.l2(scores)\n",
        "        scores = scores + x\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B49-e5xFRAl0"
      },
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.l1 = Residual(input_size)\n",
        "        self.l2 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.l3 = nn.Linear(hidden_size, output_size, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZZlIJEmXSsY"
      },
      "source": [
        "class ReplayBuffer():\n",
        "\n",
        "    def __init__(self, max_size, device):\n",
        "        self.size = max_size\n",
        "        self.device = device\n",
        "        self.states = None\n",
        "        self.actions = None\n",
        "        self.rewards = None\n",
        "        self.endstates = None\n",
        "\n",
        "    def sample(self, batchsize):\n",
        "        randidxs = torch.randperm(self.states.shape[0], device = self.device)[:batchsize]\n",
        "        return self.states[randidxs], self.actions[randidxs], self.rewards[randidxs], self.endstates[randidxs]\n",
        "\n",
        "    def add_to_array(self, array1, array2):\n",
        "        if array1:\n",
        "            tmp = torch.cat(array1, array2)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            array1 = tmp\n",
        "        else:\n",
        "            array1 = array2\n",
        "\n",
        "    def add(self, states, actions, rewards, endstates):\n",
        "        if not self.states is None:\n",
        "            tmp = torch.cat((self.states, states), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.states = tmp\n",
        "        else:\n",
        "            self.states = states\n",
        "\n",
        "        if not self.actions is None:\n",
        "            tmp = torch.cat((self.actions, actions), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.actions = tmp\n",
        "        else:\n",
        "            self.actions = actions\n",
        "\n",
        "        if not self.rewards is None:\n",
        "            tmp = torch.cat((self.rewards, rewards), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.rewards = tmp\n",
        "        else:\n",
        "            self.rewards = rewards\n",
        "\n",
        "        if not self.endstates is None:\n",
        "            tmp = torch.cat((self.endstates, endstates), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.endstates = tmp\n",
        "        else:\n",
        "            self.endstates = endstates\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6fIATmdvBYQ"
      },
      "source": [
        "# Variant B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1VRHaspDcys"
      },
      "source": [
        "class DDQNAgent:\n",
        "    # Agent class.\n",
        "\n",
        "    def __init__(self, torque_factor, action_size, state_size, action_quantize, q_module, q_init, gamma = 1.0, epsilon = 0.1, device = \"cpu\", buffersize = 10000):\n",
        "        self.torque_factor = torque_factor\n",
        "        self.action_size = action_size\n",
        "        self.action_quantize = action_quantize\n",
        "        self.state_size = state_size\n",
        "        elems = [torch.linspace(-1, 1, action_quantize, dtype=torch.float) * torque_factor] * action_size\n",
        "        self.action_space = torch.cartesian_prod(*elems).to(device)\n",
        "        self.q_module = q_module\n",
        "        self.q_init = q_init\n",
        "        self.q1 = q_module(**q_init)\n",
        "        self.q2 = copy.deepcopy(self.q1)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.device = device\n",
        "        self.buffer = ReplayBuffer(buffersize, self.device)\n",
        "        self.buffersize = buffersize\n",
        "        self.q1.to(device)\n",
        "        self.q2.to(device)\n",
        "        if \"cuda\" in device:\n",
        "            print(\"Using GPU\")\n",
        "\n",
        "\n",
        "    # First network\n",
        "    # state should be a tensor of shape [BATCH, state_size]\n",
        "    # action should be a tensor of shape [BATCH, x, action_size]\n",
        "    # return tensor will be size [BATCH, x]\n",
        "    def Q1(self, state, action):\n",
        "        return self._Q(state, action, self.q1)\n",
        "\n",
        "    # Second network\n",
        "    def Q2(self, state, action):\n",
        "        return self._Q(state, action, self.q2)\n",
        "\n",
        "    def _Q(self, state, action, q):\n",
        "        inputs = self.shape_inputs(state, action)\n",
        "        return q(inputs).squeeze(-1) \n",
        "\n",
        "    # state should be a tensor with size [BATCH, state_size]\n",
        "    # action should be a tensor with size [BATCH, x, action_size]\n",
        "    # output will be a tensor with size [BATCH, x, state_size + action_size]\n",
        "    def shape_inputs(self, state, action):\n",
        "        state = state.unsqueeze(1).repeat(1, action.shape[1], 1)\n",
        "        out = torch.cat((state, action), dim=2)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # Given a state tensor we compute the action with the highest q value\n",
        "    # Input shape is [BATCH, state_size] for sdash\n",
        "    # Output shape is [BATCH, action_size]\n",
        "    def max_action(self, q, state):\n",
        "        action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state, action)\n",
        "        max, idxs = torch.max(vals, dim=1)\n",
        "        max_action = self.action_space[idxs]\n",
        "        return max_action\n",
        "\n",
        "\n",
        "    # Given a state tensor we choose an action with the probability of chosing that action determined by the softmax of the action value.\n",
        "    def soft_action(self, q, state, temperature = 1.0):\n",
        "        action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state, action) / temperature\n",
        "        probs = F.softmax(vals, dim=1)\n",
        "        idxs = torch.multinomial(probs, 1, replacement=True)\n",
        "        soft_action = self.action_space[idxs].squeeze(1)\n",
        "        return soft_action\n",
        "\n",
        "\n",
        "    # Computes new action value targets given reward and target state\n",
        "    # reward shape should be [BATCH]\n",
        "    # state should be shape [BATCH, state_size]\n",
        "    def compute_targets(self, rewards, sdash):\n",
        "        action = self.max_action(self.Q1, sdash).unsqueeze(1)\n",
        "        action_estimate = self.Q2(sdash, action).squeeze()\n",
        "        targets = rewards + self.gamma * action_estimate\n",
        "        return targets\n",
        "\n",
        "\n",
        "    def get_experiences(self, env, n_episodes, policy):\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        endstates = []\n",
        "        # Currently we do episodes until we have n or more experiences\n",
        "        # Just get an episode and batch that?\n",
        "        for _ in range(n_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                states.append(state)\n",
        "                action = policy(state)\n",
        "                actions.append(action)\n",
        "                state, reward, done, info = env.step(action.squeeze().cpu().numpy())\n",
        "                endstate = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                rewards.append(reward)\n",
        "                endstates.append(endstate)\n",
        "        states = torch.cat(states, dim=0)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards).to(self.device)\n",
        "        endstates = torch.cat(endstates, dim=0)\n",
        "        return states, actions, rewards, endstates\n",
        "       \n",
        "\n",
        "\n",
        "    # Do the training loop for some number of gathered experiences\n",
        "    def train(self, env, train_steps, n_episodes, batch_per_step = 100, batch_size = 32, epochs = 10, lr = 0.1, t_0 = 5, t_decay = 0.99, phi_update = 5):\n",
        "        self.q1.train() # update only q1, not q2\n",
        "        self.q2.eval()\n",
        "\n",
        "        temperature = t_0\n",
        "        optimizer = torch.optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        #optimizer = torch.optim.SGD(self.q1.parameters(), lr = lr)\n",
        " \n",
        "        for i in range(train_steps):\n",
        "            print(\"Training step {}...\".format(i+1))\n",
        "            print(\"Grabbing experiences...\")\n",
        "            policy = partial(self.policy_t, temperature=temperature)\n",
        "            self.buffer.add(*self.get_experiences(env, n_episodes, policy))\n",
        "            # permute experiences and iterate by batch_size\n",
        "            #n_experiences = states.shape[0]\n",
        "            #print(states.shape, actions.shape, rewards.shape, endstates.shape)\n",
        "            #shuffled = torch.randperm(n_experiences, device = self.device)\n",
        "\n",
        "            print(\"Training... Current experience buffer size : {}\", self.buffer.states.shape[0])\n",
        "            for j in range(batch_per_step):\n",
        "                for e in range(epochs):\n",
        "                    states_b, actions_b, rewards_b, endstates_b = self.buffer.sample(batch_size)\n",
        "                    targets_b = self.compute_targets(rewards_b, endstates_b).detach()\n",
        "                    actual_b = self.Q1(states_b.detach(), actions_b.detach()).squeeze()\n",
        "\n",
        "                     # compute loss\n",
        "                    loss = F.mse_loss(actual_b, targets_b)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()  # Gradients\n",
        "                    #torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1)\n",
        "                    optimizer.step()  # do update\n",
        "                    \n",
        "                if e+1 % 10 == 0:\n",
        "                  print(\"Finished epoch {}\".format(e+1))\n",
        "            temperature = temperature * t_decay\n",
        "            if i % phi_update == 0:\n",
        "                #print(\"Updating second network\")\n",
        "                # update phi network\n",
        "                self.q2.load_state_dict(self.q1.state_dict())\n",
        "        self.q1.eval()\n",
        "        self.q2.eval()\n",
        "\n",
        "    # Given the current q functions, state, etc., give me the next action.\n",
        "    # Start with simple epsilon-greedy\n",
        "    # Alternative: softmax where we pick x with prob x\n",
        "    def policy_t(self, state, temperature = 1.0):\n",
        "        action = self.soft_action(self.Q1, state, temperature)\n",
        "        return action\n",
        "        # below is epsilon-greedy\n",
        "        #greedy_action = self.max_action(self.Q1, state)\n",
        "        #rand_idxs = torch.randint(low=0, high=self.action_space.shape[0], size=(state.shape[0],), device = self.device)\n",
        "        #rand_action = self.action_space[rand_idxs]\n",
        "        #rmask = torch.rand((state.shape[0], 1), device = self.device)\n",
        "        #out = torch.where(rmask > self.epsilon, greedy_action, rand_action)\n",
        "        #return out\n",
        "\n",
        "\n",
        "    # like policy, but takes numpy arrays\n",
        "    def policy(self, state, temperature = 1.0):\n",
        "        state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "        out = self.policy_t(state, temperature).squeeze().cpu().numpy()\n",
        "        return out\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.action_space = self.action_space.to(device) \n",
        "        self.q1 = self.q1.to(device)\n",
        "        self.q1 = self.q2.to(device)\n",
        "\n",
        "    # TODO: Function for saving/loading instances of this class\n",
        "\n",
        "    def save(self, path):\n",
        "        params = {\n",
        "            \"torque_factor\" : self.torque_factor,\n",
        "            \"action_size\" : self.action_size,\n",
        "            \"action_quantize\": self.action_quantize,\n",
        "            \"state_size\" : self.state_size,\n",
        "            \"q_module\" : self.q_module,\n",
        "            \"q_init\" : self.q_init,\n",
        "            \"gamma\" : self.gamma,\n",
        "            \"epsilon\" : self.epsilon\n",
        "        }\n",
        "        q1_state_dict = self.q1.state_dict()\n",
        "        q2_state_dict = self.q2.state_dict()\n",
        "        torch.save({\"q1_state_dict\" : q1_state_dict,\n",
        "                    \"q2_state_dict\" : q2_state_dict,\n",
        "                    \"params\" : params,\n",
        "                    \"buffer\" : self.buffer}, path)\n",
        "        \n",
        "    def load(path):\n",
        "        checkpoint = torch.load(path)\n",
        "        params = checkpoint[\"params\"]\n",
        "        newAgent = DDQNAgent(**params)\n",
        "        newAgent.buffer = checkpoint[\"buffer\"]\n",
        "        newAgent.q1.load_state_dict(checkpoint[\"q1_state_dict\"])\n",
        "        newAgent.q2.load_state_dict(checkpoint[\"q2_state_dict\"])\n",
        "        return newAgent\n",
        "\n",
        "    # TODO: Terminal states?\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDcgOB8Z9Hwp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "size_actions = 2\n",
        "size_state = 8\n",
        "torque_factor = 1 # motor torque is scaled from [-1, 1] using this factor\n",
        "\n",
        "action_quantize = 7\n",
        "gamma = 1.0\n",
        "epsilon = 0.5\n",
        "buffer_size = 1000000\n",
        "\n",
        "qnet_init_params = {\"input_size\" : size_state + size_actions, \n",
        "                    \"hidden_size\" : 64,\n",
        "                    \"output_size\" : 1}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "rl = DDQNAgent(torque_factor, size_actions, size_state, action_quantize, QNet, qnet_init_params, gamma = gamma, epsilon=epsilon, device=device, buffersize = buffer_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RO7gkI0Yd74"
      },
      "source": [
        "# Variant A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKDjQepXXEVu"
      },
      "source": [
        "class DDQNAgentVA:\n",
        "    # Agent class.\n",
        "\n",
        "    def __init__(self, torque_factor, action_size, state_size, action_quantize, q_module, q_init, gamma = 1.0, epsilon = 0.1, device = \"cpu\", buffersize = 10000):\n",
        "        self.torque_factor = torque_factor\n",
        "        self.action_size = action_size\n",
        "        self.action_quantize = action_quantize\n",
        "        self.state_size = state_size\n",
        "        elems = [torch.linspace(-1, 1, action_quantize, dtype=torch.float) * torque_factor] * action_size\n",
        "        self.action_space = torch.cartesian_prod(*elems).to(device)\n",
        "        self.q_module = q_module\n",
        "        self.q_init = q_init\n",
        "        self.q1 = q_module(**q_init, output_size = self.action_space.shape[0])\n",
        "        self.q2 = copy.deepcopy(self.q1)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.device = device\n",
        "        self.buffer = ReplayBuffer(buffersize, self.device)\n",
        "        self.buffersize = buffersize\n",
        "        self.q1.to(device)\n",
        "        self.q2.to(device)\n",
        "        if \"cuda\" in device:\n",
        "            print(\"Using GPU\")\n",
        "\n",
        "\n",
        "    # First network\n",
        "    # state should be a tensor of shape [BATCH, state_size]\n",
        "    # return tensor will be size [BATCH, action_space size]\n",
        "    def Q1(self, state):\n",
        "        return self._Q(state, self.q1)\n",
        "\n",
        "    # Second network\n",
        "    def Q2(self, state):\n",
        "        return self._Q(state, self.q2)\n",
        "\n",
        "    def _Q(self, state, q):\n",
        "        out = q(state)\n",
        "        return out\n",
        "\n",
        "    # Given a state tensor we compute the action with the highest q value\n",
        "    # Input shape is [BATCH, state_size] for state\n",
        "    # Output shape is [BATCH, action_size]\n",
        "    def max_action(self, q, state):\n",
        "        #action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state)\n",
        "        #print(\"max_action\", vals.shape)\n",
        "        max, idxs = torch.max(vals, dim=1)\n",
        "        #max_action = self.action_space[idxs]\n",
        "        #print(max_action.shape, self.action_space.shape, idxs.shape)\n",
        "        #print(idxs.shape)\n",
        "        return idxs\n",
        "\n",
        "\n",
        "    # Given a state tensor we choose an action with the probability of chosing that action determined by the softmax of the action value.\n",
        "    def soft_action(self, q, state, temperature = 1.0):\n",
        "        #action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state) / temperature\n",
        "        probs = F.softmax(vals, dim=1)\n",
        "        idxs = torch.multinomial(probs, 1, replacement=True).squeeze(1)\n",
        "        soft_action = self.action_space[idxs]\n",
        "        return soft_action\n",
        "\n",
        "\n",
        "    def get_experiences(self, env, n_episodes, policy):\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        endstates = []\n",
        "        for _ in range(n_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                states.append(state)\n",
        "                action = policy(state)\n",
        "                actions.append(action)\n",
        "                state, reward, done, info = env.step(action.squeeze().cpu().numpy())\n",
        "                endstate = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                rewards.append(reward)\n",
        "                endstates.append(endstate)\n",
        "        states = torch.cat(states, dim=0)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards).to(self.device)\n",
        "        endstates = torch.cat(endstates, dim=0)\n",
        "        return states, actions, rewards, endstates\n",
        "       \n",
        "\n",
        "\n",
        "    # Do the training loop for some number of gathered experiences\n",
        "    def train(self, \n",
        "              env, \n",
        "              train_steps, \n",
        "              n_episodes = 10, \n",
        "              batch_per_step = 100, \n",
        "              batch_size = 32, \n",
        "              epochs = 10, \n",
        "              lr = 0.1, \n",
        "              t_0 = 5, \n",
        "              t_decay = 0.99, \n",
        "              phi_update = 5, \n",
        "              save_every = 10, \n",
        "              save_dir = \"/content/drive/MyDrive/Models/trial.tar\"):\n",
        "        self.q1.train() # update only q1, not q2\n",
        "        self.q2.eval()\n",
        "\n",
        "        temperature = t_0\n",
        "        optimizer = torch.optim.Adam(self.q1.parameters(), lr=lr)\n",
        "        #optimizer = torch.optim.SGD(self.q1.parameters(), lr = lr)\n",
        " \n",
        "        for i in range(train_steps):\n",
        "            if i % 50 == 0:\n",
        "              print(\"Training step {}...\".format(i+1))\n",
        "              print(\"Current average return: {}\".format(compute_avg_return(env, rl.policy, num_episodes=10)))\n",
        "            #print(\"Grabbing experiences...\")\n",
        "            policy = partial(self.policy_t, temperature=temperature)\n",
        "            self.buffer.add(*self.get_experiences(env, n_episodes, policy))\n",
        "            # permute experiences and iterate by batch_size\n",
        "            #n_experiences = states.shape[0]\n",
        "            #print(states.shape, actions.shape, rewards.shape, endstates.shape)\n",
        "            #shuffled = torch.randperm(n_experiences, device = self.device)\n",
        "\n",
        "            #print(\"Training...\")\n",
        "            for j in range(batch_per_step):\n",
        "                for e in range(epochs):\n",
        "                    states_b, actions_b, rewards_b, endstates_b = self.buffer.sample(batch_size)\n",
        "\n",
        "                    #targets_b = self.compute_targets(rewards_b, endstates_b).detach()\n",
        "\n",
        "                    action = self.max_action(self.Q1, endstates_b)\n",
        "                    action_estimate = self.Q2(endstates_b).detach()\n",
        "\n",
        "                    action_estimate = action_estimate.gather(1, action.unsqueeze(1))\n",
        "\n",
        "                    targets_b = rewards_b.unsqueeze(1) + self.gamma * action_estimate\n",
        "\n",
        "                    actual_b = self.Q1(states_b).gather(1, action.unsqueeze(1))\n",
        "                    \n",
        "                    # compute loss\n",
        "                    loss = F.mse_loss(actual_b, targets_b)\n",
        "\n",
        "                    #loss = targets_b - actual_b\n",
        "                    #loss = - loss # flip, or network learns to crash\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    #actual_b.backward(loss.data)  # Gradients\n",
        "                    loss.backward()\n",
        "                    #torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1)\n",
        "                    optimizer.step()  # do update\n",
        "                    \n",
        "                if e+1 % 10 == 0:\n",
        "                  print(\"Finished epoch {}\".format(e+1))\n",
        "            temperature = temperature * t_decay\n",
        "            if i % phi_update == 0:\n",
        "                #print(\"Updating second network\")\n",
        "                # update phi network\n",
        "                self.q2.load_state_dict(self.q1.state_dict())\n",
        "            if i % save_every == 0:\n",
        "              self.save(save_dir)\n",
        "            \n",
        "        self.q1.eval()\n",
        "        self.q2.eval()\n",
        "\n",
        "    # Given the current q functions, state, etc., give me the next action.\n",
        "    # Start with simple epsilon-greedy\n",
        "    # Alternative: softmax where we pick x with prob x\n",
        "    def policy_t(self, state, temperature = 1.0):\n",
        "        action = self.soft_action(self.Q1, state, temperature)\n",
        "        return action\n",
        "        # below is epsilon-greedy\n",
        "        #greedy_action = self.max_action(self.Q1, state)\n",
        "        #rand_idxs = torch.randint(low=0, high=self.action_space.shape[0], size=(state.shape[0],), device = self.device)\n",
        "        #rand_action = self.action_space[rand_idxs]\n",
        "        #rmask = torch.rand((state.shape[0], 1), device = self.device)\n",
        "        #out = torch.where(rmask > self.epsilon, greedy_action, rand_action)\n",
        "        #return out\n",
        "\n",
        "\n",
        "    # like policy, but takes numpy arrays\n",
        "    def policy(self, state, temperature = 1.0):\n",
        "        state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "        out = self.policy_t(state, temperature).squeeze().cpu().numpy()\n",
        "        return out\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.action_space = self.action_space.to(device) \n",
        "        self.q1 = self.q1.to(device)\n",
        "        self.q1 = self.q2.to(device)\n",
        "\n",
        "    # TODO: Function for saving/loading instances of this class\n",
        "\n",
        "    def save(self, path):\n",
        "        params = {\n",
        "            \"torque_factor\" : self.torque_factor,\n",
        "            \"action_size\" : self.action_size,\n",
        "            \"action_quantize\": self.action_quantize,\n",
        "            \"state_size\" : self.state_size,\n",
        "            \"q_module\" : self.q_module,\n",
        "            \"q_init\" : self.q_init,\n",
        "            \"gamma\" : self.gamma,\n",
        "            \"epsilon\" : self.epsilon\n",
        "        }\n",
        "        q1_state_dict = self.q1.state_dict()\n",
        "        q2_state_dict = self.q2.state_dict()\n",
        "        torch.save({\"q1_state_dict\" : q1_state_dict,\n",
        "                    \"q2_state_dict\" : q2_state_dict,\n",
        "                    \"params\" : params,\n",
        "                    \"buffer\" : self.buffer}, path)\n",
        "        \n",
        "    def load(path):\n",
        "        checkpoint = torch.load(path)\n",
        "        params = checkpoint[\"params\"]\n",
        "        newAgent = DDQNAgentVA(**params)\n",
        "        newAgent.buffer = checkpoint[\"buffer\"]\n",
        "        newAgent.q1.load_state_dict(checkpoint[\"q1_state_dict\"])\n",
        "        newAgent.q2.load_state_dict(checkpoint[\"q2_state_dict\"])\n",
        "        return newAgent\n",
        "\n",
        "    # TODO: Terminal states?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdGQxmw56MBA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "size_actions = 2\n",
        "size_state = 8\n",
        "torque_factor = 1 # motor torque is scaled from [-1, 1] using this factor\n",
        "\n",
        "action_quantize = 7\n",
        "gamma = 1.0\n",
        "epsilon = 0.5\n",
        "buffer_size = 1000000\n",
        "\n",
        "qnet_init_params = {\"input_size\" : size_state, \n",
        "                    \"hidden_size\" : 64}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "rl = DDQNAgentVA(torque_factor, size_actions, size_state, action_quantize, QNet, qnet_init_params, gamma = gamma, epsilon=epsilon, device=device, buffersize = buffer_size)\n",
        "\n",
        "#fake_rewards = torch.randn((9,1), device = device)\n",
        "#fake_state = torch.randn((9, 8), device = device)\n",
        "#rl2.compute_targets(fake_rewards, fake_state)\n",
        "\n",
        "#rl.train(env, train_steps = 500, n_episodes = 10, batch_per_step = 100, batch_size = 32, epochs = 10, lr = 0.5, t_0 = 3, t_decay=0.998, phi_update = 5)\n",
        "#rl2.save(\"/content/drive/MyDrive/Models/trial.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhNp5-1qYkry"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTKoxlgmu6n"
      },
      "source": [
        "#rl = DDQNAgentVA.load(\"/content/drive/MyDrive/Models/trial2.tar\")\n",
        "#rl.to(device)\n",
        "dirpath = \"/content/drive/MyDrive/Models/lunarlander.tar\"\n",
        "rl.train(env, train_steps = 500, n_episodes = 10, batch_per_step = 500, batch_size = 32, epochs = 10, lr = 0.5, t_0 = 5, t_decay=0.997, phi_update = 5, save_every = 10, save_dir = dirpath)\n",
        "rl.save(dirpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPyTGJpj5-06"
      },
      "source": [
        "print(compute_avg_return(env, random_policy, num_episodes=50))\n",
        "print(compute_avg_return(env, rl.policy, num_episodes=50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            video.append_data(env.render(mode='rgb_array'))\n",
        "            while not done:\n",
        "                action = policy(observation)\n",
        "                observation, reward, done, info = env.step(action)\n",
        "                video.append_data(env.render(mode='rgb_array'))\n",
        "    return embed_mp4(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\", num_episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTAnUiPo3N1e"
      },
      "source": [
        "create_policy_eval_video(rl.policy, \"DDQN\", num_episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}