{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2021 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "To render video you will need to have ffmpeg and xvbf installed.\n",
        "Typically, installing is done with the command\n",
        "\n",
        "`sudo apt-get install -y xvfb ffmpeg`\n",
        "\n",
        "Then, if you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym[classic_control]\n",
        "!pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "source": [
        "# env_name = 'CartPole-v0'                # discrete action space\n",
        "# env_name = 'LunarLander-v2'             # discrete action space\n",
        "# env_name = 'LunarLanderContinuous-v2'   # continuous action space\n",
        "# env_name = 'BipedalWalker-v3'           # continuous action space\n",
        "env_name = 'BipedalWalkerHardcore-v3'     # continuous action space\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "You can render this environment to see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.high)\n",
        "print(env.observation_space.low)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1OEsjD43N1Y"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.reward_range)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "    \n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "\n",
        "        observation = environment.reset()\n",
        "        episode_return = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy(observation)\n",
        "            observation, reward, done, info = environment.step(action)\n",
        "            episode_return += reward\n",
        "            total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-K4TSs3N1a"
      },
      "source": [
        "def random_policy(observation):\n",
        "    return env.action_space.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bgU6Q6BZ8Bp"
      },
      "source": [
        "compute_avg_return(env, random_policy, num_episodes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrIwVRj_Q94D"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZiOx6lQ7Gq"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, input_size, bias=True)\n",
        "        #nn.init.normal_(self.linear.weight, mean = 0.0, std = sqrt(1/input_size))\n",
        "        #self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.linear(x)\n",
        "        #scores = self.relu(scores)\n",
        "        scores = scores + x\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B49-e5xFRAl0"
      },
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.l1 = Residual(input_size)\n",
        "        #self.selu = nn.SELU(inplace=True)\n",
        "        self.l2 = nn.Linear(input_size, output_size, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        #out = self.selu(out)\n",
        "        out = self.l2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1VRHaspDcys"
      },
      "source": [
        "class Agent:\n",
        "    # Agent class.\n",
        "\n",
        "    def __init__(self, torque_factor, action_size, state_size, quantize_actions, q_module, q_init, gamma = 1.0, batch_size = 32, epsilon = 0.1):\n",
        "        self.torque_factor = torque_factor\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "        self.action_space = torch.stack(action_size*[torch.linspace(-1, 1, quantize_actions, dtype=torch.float) * torque_factor])\n",
        "        self.q1 = q_module(**q_init)\n",
        "        self.q2 = copy.deepcopy(self.q1)\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "    # First network\n",
        "    # state should be a tensor of shape [BATCH, state_size]\n",
        "    # action should be a tensor of shape [BATCH, action_size ,x]\n",
        "    # return tensor will be size [BATCH, x]\n",
        "    def Q1(self, state, action):\n",
        "        return self._Q(state, action, self.q1)\n",
        "\n",
        "    # Second network\n",
        "    def Q2(self, state, action):\n",
        "        return self._Q(state, action, self.q2)\n",
        "\n",
        "    def _Q(self, state, action, q):\n",
        "        inputs = self.shape_inputs(state, action)\n",
        "        return q(inputs).squeeze(-1) \n",
        "\n",
        "    # state should be a tensor with size [BATCH, state_size]\n",
        "    # action should be a tensor with size [BATCH, action_size, x]\n",
        "    # output will be a tensor with size [BATCH, x, state_size + action_size]\n",
        "    def shape_inputs(self, state, action):\n",
        "        state = state.unsqueeze(1).repeat(1, action.shape[2], 1)\n",
        "        action = action.permute(0, 2, 1)\n",
        "        out = torch.cat((state, action), dim=2)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # Given a state tensor we compute the action with the highest q value for each state\n",
        "    # Input shape is [BATCH, state_size] for sdash\n",
        "    # Output shape is [BATCH]\n",
        "    def max_action(self, q, state):\n",
        "        action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state, action)\n",
        "        max, idxs = torch.max(vals, dim=1)\n",
        "        return max\n",
        "\n",
        "\n",
        "    # Computes new action value targets given reward and target state\n",
        "    # reward shape should be [BATCH]\n",
        "    # state should be shape [BATCH, state_size]\n",
        "    def compute_targets(self, rewards, s, sdash):\n",
        "        targets = rewards + self.gamma * self.Q1(sdash, self.max_action(self.Q2, sdash))\n",
        "\n",
        "\n",
        "    def get_experiences(self, env, n_episodes):\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        endstates = []\n",
        "        for _ in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                states.append(state)\n",
        "                action = self.policy(state)\n",
        "                actions.append(action)\n",
        "                state, reward, done, info = env.step(action)\n",
        "                rewards.append(reward)\n",
        "                endstates.append(endstate)\n",
        "                print(state, action, reward, done, info)\n",
        "        return states, actions, rewards, endstates\n",
        "\n",
        "    # Do the training loop for some number of gathered experiences\n",
        "    def train(self, states, actions, rewards, endstates):\n",
        "        # permute experiences and iterate by batch_size\n",
        "        # get targets\n",
        "        # compute loss\n",
        "        # do update\n",
        "        pass\n",
        "\n",
        "    # Given the current q functions, state, etc., give me the next action.\n",
        "    # Start with simple epsilon-greedy\n",
        "    def policy(self, state):\n",
        "        greedy_action = self.max_action(self.Q1, state)\n",
        "\n",
        "    # TODO: Function for saving/loading instances of this class\n",
        "    # TODO: Terminal states?\n",
        "\n",
        "\n",
        "size_actions = 4\n",
        "size_state = 24\n",
        "torque_factor = 1 # motor torque is scaled from [-1, 1] using this factor\n",
        "\n",
        "action_quantize = 11\n",
        "batch_size = 1\n",
        "gamma = 1.0\n",
        "\n",
        "qnet_init_params = {\"input_size\" : size_actions + size_state, \n",
        "                    \"output_size\" : 1}\n",
        "\n",
        "rl = Agent(torque_factor, size_actions, size_state, action_quantize, QNet, qnet_init_params, gamma = gamma, batch_size = batch_size)\n",
        "fake_state = torch.randn((2, size_state))\n",
        "fake_action = torch.randn((2, size_actions, 1))\n",
        "rl.Q1(fake_state, fake_action)\n",
        "rl.max_action(rl.Q1, fake_state)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            video.append_data(env.render(mode='rgb_array'))\n",
        "            while not done:\n",
        "                action = policy(observation)\n",
        "                observation, reward, done, info = env.step(action)\n",
        "                video.append_data(env.render(mode='rgb_array'))\n",
        "    return embed_mp4(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\", num_episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTAnUiPo3N1e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}