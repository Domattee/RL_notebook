{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2021 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "To render video you will need to have ffmpeg and xvbf installed.\n",
        "Typically, installing is done with the command\n",
        "\n",
        "`sudo apt-get install -y xvfb ffmpeg`\n",
        "\n",
        "Then, if you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym[classic_control]\n",
        "!pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "source": [
        "# env_name = 'CartPole-v0'                # discrete action space\n",
        "# env_name = 'LunarLander-v2'             # discrete action space\n",
        "env_name = 'LunarLanderContinuous-v2'   # continuous action space\n",
        "#env_name = 'BipedalWalker-v3'           # continuous action space\n",
        "# env_name = 'BipedalWalkerHardcore-v3'     # continuous action space\n",
        "env = gym.make(env_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "You can render this environment to see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.high)\n",
        "print(env.observation_space.low)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1OEsjD43N1Y"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.reward_range)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "    \n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "\n",
        "        observation = environment.reset()\n",
        "        episode_return = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy(observation)\n",
        "            observation, reward, done, info = environment.step(action)\n",
        "            episode_return += reward\n",
        "        total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-K4TSs3N1a"
      },
      "source": [
        "def random_policy(observation):\n",
        "    return env.action_space.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrIwVRj_Q94D"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "from functools import partial\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2OCdzxZgV8"
      },
      "source": [
        "import scipy.ndimage.filters as ndif\n",
        "def running_mean(x, N): # shamelessly taken stack exchange: https://stackoverflow.com/questions/13728392/moving-average-or-running-mean/43200476#43200476\n",
        "    return ndif.uniform_filter1d(x, N, mode='constant', origin=-(N//2))[:-(N-1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZiOx6lQ7Gq"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(input_size, input_size, bias=True)\n",
        "        self.l2 = nn.Linear(input_size, input_size, bias=True)\n",
        "        #nn.init.normal_(self.linear.weight, mean = 0.0, std = sqrt(1/input_size))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.l1(x)\n",
        "        scores = self.relu(scores)\n",
        "        scores = self.l2(scores)\n",
        "        scores = scores + x\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B49-e5xFRAl0"
      },
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.l1 = Residual(input_size)\n",
        "        #self.lstm = nn.LSTM(input_size = input_size, hidden_size = 100, num_layers=1) # This will require a whole load of work\n",
        "        self.l2 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.l3 = nn.Linear(hidden_size, output_size, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        #self.batchnormIn = nn.BatchNorm1d(input_size)\n",
        "        #self.batchnormRes = nn.BatchNorm1d(input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #out = self.batchnormIn(x)\n",
        "        out = self.l1(x)\n",
        "        #out = self.batchnormRes(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZZlIJEmXSsY"
      },
      "source": [
        "class ReplayBuffer():\n",
        "\n",
        "    def __init__(self, max_size, device):\n",
        "        self.size = max_size\n",
        "        self.device = device\n",
        "        self.states = None\n",
        "        self.actions = None\n",
        "        self.rewards = None\n",
        "        self.endstates = None\n",
        "        self.done = None\n",
        "\n",
        "\n",
        "    def sample(self, batchsize):\n",
        "        randidxs = torch.randperm(self.states.shape[0], device = self.device)[:batchsize]\n",
        "        return self.states[randidxs], self.actions[randidxs], self.rewards[randidxs], self.endstates[randidxs], self.done[randidxs]\n",
        "\n",
        "\n",
        "    def cur_size(self):\n",
        "        if self.states is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return self.states.shape[0]\n",
        "\n",
        "\n",
        "    def add(self, states, actions, rewards, endstates, done):\n",
        "        if not self.states is None:\n",
        "            tmp = torch.cat((self.states, states), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.states = tmp\n",
        "        else:\n",
        "            self.states = states\n",
        "\n",
        "        if not self.actions is None:\n",
        "            tmp = torch.cat((self.actions, actions), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.actions = tmp\n",
        "        else:\n",
        "            self.actions = actions\n",
        "\n",
        "        if not self.rewards is None:\n",
        "            tmp = torch.cat((self.rewards, rewards), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.rewards = tmp\n",
        "        else:\n",
        "            self.rewards = rewards\n",
        "\n",
        "        if not self.endstates is None:\n",
        "            tmp = torch.cat((self.endstates, endstates), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.endstates = tmp\n",
        "        else:\n",
        "            self.endstates = endstates\n",
        "\n",
        "        if not self.done is None:\n",
        "            tmp = torch.cat((self.done, done), dim = 0)\n",
        "            if tmp.shape[0] > self.size:\n",
        "                tmp = tmp[tmp.shape[0] - self.size:]\n",
        "            self.done = tmp\n",
        "        else:\n",
        "            self.done = done\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6fIATmdvBYQ"
      },
      "source": [
        "# Variant B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1VRHaspDcys"
      },
      "source": [
        "class DDQNAgent:\n",
        "    # Agent class.\n",
        "\n",
        "    def __init__(self, \n",
        "                 torque_factor, \n",
        "                 action_size, \n",
        "                 action_space,\n",
        "                 state_size, \n",
        "                 q_module, \n",
        "                 q_init, \n",
        "                 gamma = 1.0, \n",
        "                 epsilon = 0.1, \n",
        "                 device = \"cpu\", \n",
        "                 buffersize = 10000, \n",
        "                 cur_best_score = -math.inf,\n",
        "                 lr = 0.01,\n",
        "                 clip = False):\n",
        "        self.torque_factor = torque_factor\n",
        "        self.action_size = action_size\n",
        "        self.action_space = action_space\n",
        "        self.state_size = state_size\n",
        "        self.q_module = q_module\n",
        "        self.q_init = q_init\n",
        "        self.q1 = q_module(**q_init)\n",
        "        self.q2 = copy.deepcopy(self.q1)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.device = device\n",
        "        self.buffer = ReplayBuffer(buffersize, self.device)\n",
        "        self.buffersize = buffersize\n",
        "        self.q1.to(device)\n",
        "        self.q2.to(device)\n",
        "        self.lr = lr\n",
        "        self.best = cur_best_score\n",
        "        self.clip = clip\n",
        "        self.optimizer = torch.optim.RMSprop(self.q1.parameters(), lr=lr)\n",
        "        if \"cuda\" in device:\n",
        "            print(\"Using GPU\")\n",
        "\n",
        "\n",
        "    # First network\n",
        "    # state should be a tensor of shape [BATCH, state_size]\n",
        "    # action should be a tensor of shape [BATCH, x, action_size]\n",
        "    # return tensor will be size [BATCH, x]\n",
        "    def Q1(self, state, action):\n",
        "        return self._Q(state, action, self.q1)\n",
        "\n",
        "    # Second network\n",
        "    def Q2(self, state, action):\n",
        "        return self._Q(state, action, self.q2)\n",
        "\n",
        "    def _Q(self, state, action, q):\n",
        "        inputs = self.shape_inputs(state, action)\n",
        "        return q(inputs).squeeze(-1) \n",
        "\n",
        "    # state should be a tensor with size [BATCH, state_size]\n",
        "    # action should be a tensor with size [BATCH, x, action_size]\n",
        "    # output will be a tensor with size [BATCH, x, state_size + action_size]\n",
        "    def shape_inputs(self, state, action):\n",
        "        state = state.unsqueeze(1).repeat(1, action.shape[1], 1)\n",
        "        out = torch.cat((state, action), dim=2)\n",
        "        return out\n",
        "\n",
        "    # Given a state tensor we compute the action with the highest q value\n",
        "    # Input shape is [BATCH, state_size] for sdash\n",
        "    # Output shape is [BATCH, action_size]\n",
        "    def max_action(self, q, state):\n",
        "        action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state, action)\n",
        "        max, idxs = torch.max(vals, dim=1)\n",
        "        max_action = self.action_space[idxs]\n",
        "        return max_action\n",
        "\n",
        "\n",
        "    # Given a state tensor we choose an action with the probability of chosing that action determined by the softmax of the action value.\n",
        "    # state should be shape [BATCH, state_size]\n",
        "    def soft_action(self, q, state, temperature = 1.0):\n",
        "        action = self.action_space.unsqueeze(0).repeat(state.shape[0], 1,1)\n",
        "        vals = q(state, action) / temperature\n",
        "        probs = F.softmax(vals, dim=1)\n",
        "        idxs = torch.multinomial(probs, 1, replacement=True)\n",
        "        soft_action = self.action_space[idxs].squeeze(1)\n",
        "        return soft_action, idxs\n",
        "\n",
        "\n",
        "    # Computes new action value targets given reward and target state\n",
        "    # reward shape should be [BATCH]\n",
        "    # state should be shape [BATCH, state_size]\n",
        "    def compute_targets(self, rewards, sdash, dones):\n",
        "        action = self.max_action(self.Q1, sdash).unsqueeze(1)\n",
        "        dones_invert = dones.detach().clone()\n",
        "        dones_invert[dones < 0.5] == 1.0\n",
        "        dones_invert[dones >= 0.5] == 0.0\n",
        "        action_estimate = self.Q2(sdash, action).squeeze()\n",
        "        targets = rewards + self.gamma * action_estimate * dones_invert\n",
        "        return targets\n",
        "\n",
        "\n",
        "    def get_experiences(self, env, n_episodes, policy):\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        endstates = []\n",
        "        dones = []\n",
        "        for _ in range(n_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                states.append(state)\n",
        "                action, idxs = policy(state)\n",
        "                actions.append(action)\n",
        "                state, reward, done, info = env.step(action.squeeze().cpu().numpy())\n",
        "                endstate = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                rewards.append(reward)\n",
        "                endstates.append(endstate)\n",
        "                dones.append(done)\n",
        "        states = torch.cat(states, dim=0)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards).to(self.device)\n",
        "        endstates = torch.cat(endstates, dim=0)\n",
        "        dones = torch.Tensor(dones).to(self.device)\n",
        "        return states, actions, rewards, endstates, dones\n",
        "       \n",
        "\n",
        "    # Do the training loop for some number of gathered experiences\n",
        "    def train(self, \n",
        "              env, \n",
        "              train_steps, \n",
        "              n_episodes = 10, \n",
        "              batch_per_step = 100, \n",
        "              batch_size = 32, \n",
        "              epochs = 10, \n",
        "              lr = None, \n",
        "              t_0 = 5, \n",
        "              t_decay = 0.99, \n",
        "              phi_update = 5, \n",
        "              save_every = 10, \n",
        "              save_dir = \"/content/drive/MyDrive/Models/trial.tar\", \n",
        "              train_after = 50000,\n",
        "              reset = False,\n",
        "              clip = None):\n",
        "        self.q1.train() # update only q1, not q2\n",
        "        self.q2.eval()\n",
        "\n",
        "        if lr is None:\n",
        "            lr = self.lr\n",
        "\n",
        "        if not clip is None:\n",
        "            self.clip = clip\n",
        "\n",
        "        if reset:\n",
        "            self.best = - math.inf\n",
        "            self.optimizer = torch.optim.RMSprop(self.q1.parameters(), lr=lr)\n",
        "\n",
        "        rewardlist = []\n",
        "\n",
        "        temperature = t_0\n",
        " \n",
        "        print(\"Grabbing initial experiences...\")\n",
        "        while(self.buffer.cur_size() < train_after):\n",
        "            self.buffer.add(*self.get_experiences(env, 1, self.policy_random))\n",
        " \n",
        "        for i in range(train_steps):\n",
        "            \n",
        "            #print(\"Grabbing experiences...\")\n",
        "            policy = partial(self.policy_t, temperature=temperature)\n",
        "            if i % save_every == 0:\n",
        "              print(\"Training step {}...\".format(i))\n",
        "            states, actions, rewards, endstates, dones = self.get_experiences(env, n_episodes, policy)\n",
        "            rewardlist.append(rewards)\n",
        "            self.buffer.add(states, actions, rewards, endstates, dones)\n",
        "\n",
        "\n",
        "            for j in range(batch_per_step):\n",
        "                for e in range(epochs):\n",
        "                    states_b, actions_b, rewards_b, endstates_b, dones_b = self.buffer.sample(batch_size)\n",
        "                    if self.clip:\n",
        "                      rewards_b = torch.clip(rewards_b, -1, 1)\n",
        "                    targets_b = self.compute_targets(rewards_b, endstates_b, dones_b).detach()\n",
        "                    print(targets_b)\n",
        "                    actual_b = self.Q1(states_b.detach(), actions_b.detach()).squeeze()\n",
        "\n",
        "                     # compute loss\n",
        "                    loss = F.mse_loss(actual_b, targets_b)\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()  # Gradients\n",
        "                    #torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1)\n",
        "                    self.optimizer.step()  # do update\n",
        "                    \n",
        "                #if e+1 % 10 == 0:\n",
        "                #  print(\"Finished epoch {}\".format(e+1))\n",
        "            temperature = temperature * t_decay\n",
        "            if i % phi_update == 0:\n",
        "                #print(\"Updating second network\")\n",
        "                # update phi network\n",
        "                self.q2.load_state_dict(self.q1.state_dict())\n",
        "            if i % save_every == 0:\n",
        "              score = compute_avg_return(env, partial(rl.policy, temperature = 1), num_episodes=20)\n",
        "              print(\"Current average return: {}\".format(score))\n",
        "              print(\"Current average return with very greedy: {}\".format(compute_avg_return(env, partial(rl.policy, temperature = 0.1), num_episodes=20)))\n",
        "              if score > self.best:\n",
        "                  self.best = score\n",
        "                  self.save(save_dir)\n",
        "                  print(\"Saving...\")\n",
        "        self.q1.eval()\n",
        "        self.q2.eval()\n",
        "        return rewardlist\n",
        "\n",
        "    # Given the current q functions, state, etc., give me the next action.\n",
        "    # Start with simple epsilon-greedy\n",
        "    # Alternative: softmax where we pick x with prob x\n",
        "    def policy_t(self, state, temperature = 1.0):\n",
        "        action, idxs = self.soft_action(self.Q1, state, temperature)\n",
        "        return action, idxs\n",
        "\n",
        "\n",
        "    def policy_random(self, state, temperature = 0):\n",
        "        randidxs = torch.randint(low = 0, high = self.action_space.shape[0], size = (state.shape[0],), device = self.device)\n",
        "        action = self.action_space[randidxs]\n",
        "        return action, randidxs\n",
        "\n",
        "    # like policy, but takes numpy arrays\n",
        "    def policy(self, state, temperature = 1.0):\n",
        "        state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "        action, idxs = self.policy_t(state, temperature)\n",
        "        out = action.squeeze().cpu().numpy()\n",
        "        return out\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.action_space = self.action_space.to(device) \n",
        "        self.q1 = self.q1.to(device)\n",
        "        self.q1 = self.q2.to(device)\n",
        "\n",
        "    # TODO: Function for saving/loading instances of this class\n",
        "\n",
        "    def save(self, path):\n",
        "        params = {\n",
        "            \"torque_factor\" : self.torque_factor,\n",
        "            \"action_size\" : self.action_size,\n",
        "            \"action_space\": self.action_space,\n",
        "            \"state_size\" : self.state_size,\n",
        "            \"q_module\" : self.q_module,\n",
        "            \"q_init\" : self.q_init,\n",
        "            \"gamma\" : self.gamma,\n",
        "            \"epsilon\" : self.epsilon,\n",
        "            \"cur_best_score\": self.best,\n",
        "            \"clip\" : self.clip\n",
        "        }\n",
        "        torch.save({\"q1_state_dict\" : self.q1.state_dict(),\n",
        "                    \"q2_state_dict\" : self.q2.state_dict(),\n",
        "                    \"optim_state_dict\" : self.optimizer.state_dict(),\n",
        "                    \"params\" : params,\n",
        "                    \"buffer\" : self.buffer}, path)\n",
        "        \n",
        "\n",
        "    def load(path):\n",
        "        checkpoint = torch.load(path)\n",
        "        params = checkpoint[\"params\"]\n",
        "        newAgent = DDQNAgent(**params)\n",
        "        newAgent.buffer = checkpoint[\"buffer\"]\n",
        "        newAgent.q1.load_state_dict(checkpoint[\"q1_state_dict\"])\n",
        "        newAgent.q2.load_state_dict(checkpoint[\"q2_state_dict\"])\n",
        "        newAgent.optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
        "        return newAgent\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDcgOB8Z9Hwp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "size_actions = 2 \n",
        "size_state = 8 \n",
        "torque_factor = 1 # motor torque is scaled from [-1, 1] using this factor\n",
        "\n",
        "action_quantize = 7\n",
        "elems = [torch.linspace(-1, 1, action_quantize, dtype=torch.float) * torque_factor] * size_actions\n",
        "action_space = torch.cartesian_prod(*elems).to(device)\n",
        "\n",
        "\n",
        "gamma = 0.95\n",
        "epsilon = 0.5\n",
        "buffer_size = 1000000\n",
        "clip = False\n",
        "lr = 0.005\n",
        "\n",
        "qnet_init_params = {\"input_size\" : size_state + size_actions, \n",
        "                    \"hidden_size\" : 64,\n",
        "                    \"output_size\" : 1}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "rl = DDQNAgent(torque_factor, size_actions, action_space, size_state, QNet, qnet_init_params, gamma = gamma, epsilon=epsilon, device=device, buffersize = buffer_size, lr = lr, clip = clip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RO7gkI0Yd74"
      },
      "source": [
        "# Variant A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKDjQepXXEVu"
      },
      "source": [
        "class DDQNAgentVA:\n",
        "    # Agent class.\n",
        "\n",
        "    def __init__(self, \n",
        "                 torque_factor, \n",
        "                 action_size, \n",
        "                 action_space,\n",
        "                 state_size, \n",
        "                 q_module, \n",
        "                 q_init, \n",
        "                 gamma = 1.0, \n",
        "                 epsilon = 0.1, \n",
        "                 device = \"cpu\", \n",
        "                 buffersize = 10000, \n",
        "                 cur_best_score = -math.inf,\n",
        "                 lr = 0.01,\n",
        "                 clip = False):\n",
        "        self.torque_factor = torque_factor\n",
        "        self.action_size = action_size\n",
        "        self.state_size = state_size\n",
        "        self.action_space = action_space\n",
        "        self.q_module = q_module\n",
        "        self.q_init = q_init\n",
        "        self.q1 = q_module(**q_init, output_size = self.action_space.shape[0])\n",
        "        self.q2 = copy.deepcopy(self.q1)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.device = device\n",
        "        self.buffer = ReplayBuffer(buffersize, self.device)\n",
        "        self.buffersize = buffersize\n",
        "        self.best = cur_best_score\n",
        "        self.q1.to(device)\n",
        "        self.q2.to(device)\n",
        "        self.lr = lr\n",
        "        self.optimizer = torch.optim.RMSprop(self.q1.parameters(), lr=lr)\n",
        "        self.clip = clip\n",
        "        if \"cuda\" in device:\n",
        "            print(\"Using GPU\")\n",
        "\n",
        "\n",
        "    # First network\n",
        "    # state should be a tensor of shape [BATCH, state_size]\n",
        "    # return tensor will be size [BATCH, action_space size]\n",
        "    def Q1(self, state):\n",
        "        return self._Q(state, self.q1)\n",
        "\n",
        "    # Second network\n",
        "    def Q2(self, state):\n",
        "        return self._Q(state, self.q2)\n",
        "\n",
        "    def _Q(self, state, q):\n",
        "        out = q(state)\n",
        "        return out\n",
        "\n",
        "    # Given a state tensor we compute the action with the highest q value\n",
        "    # Input shape is [BATCH, state_size] for state\n",
        "    # Output shape is [BATCH, action_size]\n",
        "    def max_action(self, q, state):\n",
        "        vals = q(state)\n",
        "        max, idxs = torch.max(vals, dim=1)\n",
        "        return idxs\n",
        "\n",
        "    # Given a state tensor we choose an action with the probability of chosing that action determined by the softmax of the action value.\n",
        "    def soft_action(self, q, state, temperature = 1.0):\n",
        "        vals = q(state) / temperature\n",
        "        probs = F.softmax(vals, dim=1)\n",
        "        idxs = torch.multinomial(probs, 1, replacement=True).squeeze(1)\n",
        "        soft_action = self.action_space[idxs]\n",
        "        return soft_action, idxs\n",
        "\n",
        "    def get_experiences(self, env, n_episodes, policy):\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        endstates = []\n",
        "        dones = []\n",
        "        for _ in range(n_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                states.append(state)\n",
        "                action, idxs = policy(state)\n",
        "                actions.append(idxs)\n",
        "                state, reward, done, info = env.step(action.squeeze().cpu().numpy())\n",
        "                dones.append(done)\n",
        "                endstate = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "                rewards.append(reward)\n",
        "                endstates.append(endstate)\n",
        "        states = torch.cat(states, dim=0)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards).to(self.device)\n",
        "        endstates = torch.cat(endstates, dim=0)\n",
        "        dones = torch.Tensor(dones).to(self.device)\n",
        "        return states, actions, rewards, endstates, dones\n",
        "\n",
        "\n",
        "    # Do the training loop for some number of gathered experiences\n",
        "    def train(self, \n",
        "              env, \n",
        "              train_steps, \n",
        "              n_episodes = 10, \n",
        "              batch_per_step = 100, \n",
        "              batch_size = 32, \n",
        "              epochs = 10, \n",
        "              lr = None, \n",
        "              t_0 = 5, \n",
        "              t_decay = 0.99, \n",
        "              phi_update = 5, \n",
        "              save_every = 10, \n",
        "              save_dir = \"/content/drive/MyDrive/Models/trial.tar\", \n",
        "              train_after = 50000,\n",
        "              reset = False,\n",
        "              clip = None):\n",
        "        # update only q1, not q2\n",
        "        self.q2.eval()\n",
        "        \n",
        "        rewardlist = []\n",
        "        temperature_returns = []\n",
        "        v_greedy_returns = []\n",
        "\n",
        "        if lr is None:\n",
        "            lr = self.lr\n",
        "\n",
        "        if not clip is None:\n",
        "            self.clip = clip\n",
        "\n",
        "        if reset:\n",
        "            self.best = - math.inf\n",
        "            self.optimizer = torch.optim.RMSprop(self.q1.parameters(), lr=lr)\n",
        "\n",
        "        temperature = t_0\n",
        "        #optimizer = torch.optim.Adam(self.q1.parameters(), lr=lr)\n",
        "       \n",
        "        #optimizer = torch.optim.SGD(self.q1.parameters(), lr = lr)\n",
        "\n",
        "        print(\"Grabbing initial experiences...\")\n",
        "        while(self.buffer.cur_size() < train_after):\n",
        "            \n",
        "            self.buffer.add(*self.get_experiences(env, 1, self.policy_random))\n",
        " \n",
        "        for i in range(train_steps):\n",
        "            \n",
        "            #print(\"Grabbing experiences...\")\n",
        "            policy = partial(self.policy_t, temperature=temperature)\n",
        "            if i % save_every == 0:\n",
        "              print(\"Training step {}...\".format(i))\n",
        "            self.q1.eval()\n",
        "            states, actions, rewards, endstates, dones = self.get_experiences(env, n_episodes, policy)\n",
        "            self.q1.train()\n",
        "            rewardlist.append(rewards)\n",
        "            self.buffer.add(states, actions, rewards, endstates, dones)\n",
        "\n",
        "            #print(\"Training...\")\n",
        "            for j in range(batch_per_step):\n",
        "                for e in range(epochs):\n",
        "                    states_b, actions_b, rewards_b, endstates_b, dones_b = self.buffer.sample(batch_size)\n",
        "                    if self.clip:\n",
        "                        rewards_b = torch.clip(rewards_b, -1, 1)\n",
        "                    action = self.max_action(self.Q1, endstates_b)\n",
        "                    action_estimate = self.Q2(endstates_b).gather(1, action.unsqueeze(1))\n",
        "                    dones_b_invert = dones_b.clone()\n",
        "                    dones_b_invert[dones_b >= 0.5] = 0\n",
        "                    dones_b_invert[dones_b < 0.5] = 1.0\n",
        "                    targets_b = rewards_b.unsqueeze(1) + self.gamma * action_estimate * dones_b_invert.unsqueeze(1)\n",
        "\n",
        "                    actual_b = self.Q1(states_b.detach()).gather(1, actions_b.detach())\n",
        "                    \n",
        "                    # compute loss\n",
        "                    loss = F.mse_loss(actual_b, targets_b.detach())\n",
        "\n",
        "                    #loss = targets_b - actual_b\n",
        "                    #loss = - loss # flip, or network learns to crash\n",
        "                    \n",
        "                    self.optimizer.zero_grad()\n",
        "                    #actual_b.backward(loss.data)  # Gradients\n",
        "                    loss.backward()\n",
        "                    #torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1)\n",
        "                    self.optimizer.step()  # do update\n",
        "                    \n",
        "                #if e+1 % 10 == 0:\n",
        "                #  print(\"Finished epoch {}\".format(e+1))\n",
        "            \n",
        "            if i % phi_update == 0:\n",
        "                #print(\"Updating second network\")\n",
        "                # update phi network\n",
        "                self.q2.load_state_dict(self.q1.state_dict())\n",
        "            if i % save_every == 0:\n",
        "              self.q1.eval()\n",
        "              score = compute_avg_return(env, partial(rl.policy, temperature = temperature), num_episodes=5)\n",
        "              greedy_score = compute_avg_return(env, partial(rl.policy, temperature = 0.01), num_episodes=5)\n",
        "              self.q1.train()\n",
        "              temperature_returns.append(score)\n",
        "              v_greedy_returns.append(greedy_score)\n",
        "              print(\"Current average return: {}\".format(score))\n",
        "              print(\"Current average return with very greedy: {}\".format(greedy_score))\n",
        "              # should save using greedy score probably?\n",
        "              if score > self.best:\n",
        "                  self.best = score\n",
        "                  self.save(save_dir)\n",
        "                  print(\"Saving...\")\n",
        "            temperature = temperature * t_decay\n",
        "\n",
        "        self.q1.eval()\n",
        "        self.q2.eval()\n",
        "        return rewardlist, temperature_returns, v_greedy_returns\n",
        "\n",
        "    # Given the current q functions, state, etc., give me the next action.\n",
        "    # Start with simple epsilon-greedy\n",
        "    # Alternative: softmax where we pick x with prob x\n",
        "    def policy_t(self, state, temperature = 1.0):\n",
        "        action, idx = self.soft_action(self.Q1, state, temperature)\n",
        "        return action, idx\n",
        "        # below is epsilon-greedy\n",
        "        #greedy_action = self.max_action(self.Q1, state)\n",
        "        #rand_idxs = torch.randint(low=0, high=self.action_space.shape[0], size=(state.shape[0],), device = self.device)\n",
        "        #rand_action = self.action_space[rand_idxs]\n",
        "        #rmask = torch.rand((state.shape[0], 1), device = self.device)\n",
        "        #out = torch.where(rmask > self.epsilon, greedy_action, rand_action)\n",
        "        #return out\n",
        "\n",
        "    def policy_random(self, state, temperature = 0):\n",
        "        randidxs = torch.randint(low = 0, high = self.action_space.shape[0], size = (state.shape[0],), device = self.device)\n",
        "        action = self.action_space[randidxs]\n",
        "        return action, randidxs\n",
        "\n",
        "\n",
        "    # like policy, but takes numpy arrays\n",
        "    def policy(self, state, temperature = 1.0):\n",
        "        state = torch.Tensor(state).unsqueeze(0).to(self.device)\n",
        "        action, idx = self.policy_t(state, temperature)\n",
        "        out = action.squeeze().cpu().numpy()\n",
        "        return out\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.action_space = self.action_space.to(device) \n",
        "        self.q1 = self.q1.to(device)\n",
        "        self.q1 = self.q2.to(device)\n",
        "\n",
        "\n",
        "    def save(self, path):\n",
        "        params = {\n",
        "            \"torque_factor\" : self.torque_factor,\n",
        "            \"action_size\" : self.action_size,\n",
        "            \"action_space\": self.action_space,\n",
        "            \"state_size\" : self.state_size,\n",
        "            \"q_module\" : self.q_module,\n",
        "            \"q_init\" : self.q_init,\n",
        "            \"gamma\" : self.gamma,\n",
        "            \"epsilon\" : self.epsilon,\n",
        "            \"cur_best_score\": self.best,\n",
        "            \"clip\" : self.clip,\n",
        "            \"lr\" : self.lr\n",
        "        }\n",
        "        torch.save({\"q1_state_dict\" : self.q1.state_dict(),\n",
        "                    \"q2_state_dict\" : self.q2.state_dict(),\n",
        "                    \"optim_state_dict\" : self.optimizer.state_dict(),\n",
        "                    \"params\" : params,\n",
        "                    \"buffer\" : self.buffer}, path)\n",
        "        \n",
        "    def load(path):\n",
        "        checkpoint = torch.load(path)\n",
        "        params = checkpoint[\"params\"]\n",
        "        newAgent = DDQNAgentVA(**params)\n",
        "        newAgent.buffer = checkpoint[\"buffer\"]\n",
        "        newAgent.q1.load_state_dict(checkpoint[\"q1_state_dict\"])\n",
        "        newAgent.q2.load_state_dict(checkpoint[\"q2_state_dict\"])\n",
        "        newAgent.optimizer.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
        "        return newAgent\n",
        "\n",
        "    # TODO: Terminal states?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdGQxmw56MBA"
      },
      "source": [
        "size_actions = 2\n",
        "size_state = 8\n",
        "torque_factor = 1 # motor torque is scaled from [-1, 1] using this factor\n",
        "\n",
        "action_quantize = 7\n",
        "elems = [torch.linspace(-1, 1, action_quantize, dtype=torch.float) * torque_factor] * size_actions\n",
        "action_space = torch.cartesian_prod(*elems).to(device)\n",
        "\n",
        "gamma = 0.95\n",
        "epsilon = 0.01 # currently does nothing with softmax policy\n",
        "buffer_size = 1000000\n",
        "clip = False\n",
        "lr = 0.001 # 0.001\n",
        "\n",
        "qnet_init_params = {\"input_size\" : size_state, \n",
        "                    \"hidden_size\" : 64}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "rl = DDQNAgentVA(torque_factor, size_actions, action_space, size_state, QNet, qnet_init_params, gamma = gamma, epsilon=epsilon, device=device, buffersize = buffer_size, lr = lr, clip = clip)\n",
        "\n",
        "#fake_rewards = torch.randn((9,1), device = device)\n",
        "#fake_state = torch.randn((9, 8), device = device)\n",
        "#rl2.compute_targets(fake_rewards, fake_state)\n",
        "\n",
        "#rl.train(env, train_steps = 500, n_episodes = 10, batch_per_step = 100, batch_size = 32, epochs = 10, lr = 0.5, t_0 = 3, t_decay=0.998, phi_update = 5)\n",
        "#rl2.save(\"/content/drive/MyDrive/Models/trial.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhNp5-1qYkry"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTKoxlgmu6n"
      },
      "source": [
        "dirpath = \"/content/drive/MyDrive/Models/temp.tar\"\n",
        "#del rl\n",
        "#checkpoint = torch.load(dirpath)\n",
        "#print(checkpoint[\"params\"])\n",
        "#rl = DDQNAgentVA.load(dirpath)\n",
        "#rl.to(device)\n",
        "rewardlist, temperature_returns, v_greedy_returns = rl.train(env,\n",
        "         train_steps =  501,\n",
        "         n_episodes = 1,\n",
        "         train_after = 50000,\n",
        "         batch_per_step = 200,\n",
        "         batch_size = 256, # 32\n",
        "         epochs = 1,\n",
        "         #lr = 0.01, \n",
        "         #clip = True,\n",
        "         t_0 = 1,\n",
        "         t_decay= 1,\n",
        "         phi_update = 10,\n",
        "         save_every = 10,\n",
        "         save_dir = dirpath)\n",
        "#rl.save(dirpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPyTGJpj5-06"
      },
      "source": [
        "#print(compute_avg_return(env, random_policy, num_episodes=10)) # walker -18000\n",
        "#print(compute_avg_return(env, rl.policy, num_episodes=10))\n",
        "#print(compute_avg_return(env, partial(rl.policy, temperature = 0.01), num_episodes=10))\n",
        "\n",
        "plotreturn = np.asarray([torch.sum(entry).item() for entry in rewardlist])\n",
        "plottemp = np.asarray(temperature_returns)\n",
        "plotgreedy = np.asarray(v_greedy_returns)\n",
        "# Store data for plots\n",
        "basedir = \"/content/drive/MyDrive/Models/plotdata\"\n",
        "np.save(os.path.join(basedir, \"returns_bs256_lr0-001_gamma095_3\"), plotreturn)\n",
        "np.save(os.path.join(basedir, \"temp_bs256_lr0-001_gamma095_3\"), plottemp)\n",
        "np.save(os.path.join(basedir, \"greedy_bs256_lr0-001_gamma095_3\"), plotgreedy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPCzQRZwbW3m"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plotrewards = [torch.sum(entry).item() for entry in rewardlist]\n",
        "print(len(plotrewards))\n",
        "#rint(len(rewardlist))\n",
        "#print(torch.sum(rewardlist[0]))\n",
        "N = 50\n",
        "r_mean = running_mean(plotrewards, N)\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(range(len(rewardlist)), plotrewards , alpha = 0.2, c = \"b\", label=\"actual returns\")\n",
        "plt.plot(range(N//2, N//2+len(r_mean)), r_mean, c=\"b\", label = \"moving average\")\n",
        "plt.xlabel(\"Training episodes\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            video.append_data(env.render(mode='rgb_array'))\n",
        "            while not done:\n",
        "                action = policy(observation)\n",
        "                observation, reward, done, info = env.step(action)\n",
        "                video.append_data(env.render(mode='rgb_array'))\n",
        "    return embed_mp4(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\", num_episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTAnUiPo3N1e"
      },
      "source": [
        "create_policy_eval_video(partial(rl.policy, temperature=1), \"DDQN\", num_episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIPe65U6K8Qs"
      },
      "source": [
        "create_policy_eval_video(partial(rl.policy, temperature=0.01), \"DDQN\", num_episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}